---
title: "Lab 4- Cloud Identification"
author: "Hoang Duong, Rahul Verma, Andre Waschka"
date: "November 7, 2014"
output:
  html_document: default
  pdf_document:
    fig_caption: yes
---

#1. Introduction

When it comes to understanding and predicting global climate change, identification of cloud cover is a vital portion of the equation. To do this, scientists create algorithms using satellite images to distinguish between clouds and non-clouds. Since most of these models use the reflecting light from the sun as a primary distinguisher, clouds are easy to identify because they reflect light at a much higher rate than land or ocean. However, an issue arises when trying to identify cloud cover in polar regions. This is due to the snow and ice reflecting sun in a similar way to clouds and thus making differentiation much more difficult. Our goal is to use the few images that we have that were labeled by an expert to train a model to be able to identify clouds vs non-clouds in polar regions.

#2. Exploratory Data Analysis

```{r,echo=FALSE,message=FALSE}
library(knitr)
#opts_chunk$set(dev="png")
library(dplyr)
library(ggplot2)
library(corrplot)
library(glmnet)
library(jpeg)

auc = glmnet::auc

setwd("~/Dropbox/School/ST215/Lab5Fixed/Stats-215---Lab-4/")
source("helper.R")
# SaveTrainTestToFile()
# Get the data for three images
image1 <- read.table('image1.txt', header=F)
image2 <- read.table('image2.txt', header=F)
image3 <- read.table('image3.txt', header=F)

# Add informative column names
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
names(image1) <- collabs
names(image2) <- collabs
names(image3) <- collabs
rm(collabs)

load("test.RData")
load("train.RData")


```

```{r,echo=FALSE,fig.cap="Heat and correlation plot of the training data combined to visually and quantitatively see the relationships between the variables."}
par(mfrow=c(1,1))
traincor<-train[,-(1:2),drop=FALSE]
traincor$blockid<-NULL

N<- cor(traincor)
corrplot.mixed(N)

```


```{r,echo=FALSE, fig.width=2, fig.height=2}
#For training data
# Class conditional densities.

cloud<- label <-train$label
cloud<- 2*label - 1
a = scale_fill_discrete(guide = guide_legend(title = "Cloud"))
b = guides(fill = FALSE)
c = theme(legend.position="bottom")
ggplot(train) + geom_density(aes(x=DF, group=factor(label), fill=factor(cloud)), alpha=0.5) + b
ggplot(train) + geom_density(aes(x=CF, group=factor(label), fill=factor(cloud)), alpha=0.5) + b
ggplot(train) + geom_density(aes(x=BF, group=factor(label), fill=factor(cloud)), alpha=0.5) + b
ggplot(train) + geom_density(aes(x=AF, group=factor(label), fill=factor(cloud)), alpha=0.5) + b

```

```{r,echo=FALSE, fig.cap =  "Class conditional density plots of the training data on the five camera angles and SD, CORR, and NDAI", fig.width=2, fig.height=2.5}
ggplot(train) + geom_density(aes(x=AN, group=factor(label), fill=factor(cloud)), alpha=0.5) + a + c
ggplot(train) + geom_density(aes(x=NDAI, group=factor(label), fill=factor(cloud)), alpha=0.5) + a + c
ggplot(train) + geom_density(aes(x=SD, group=factor(label), fill=factor(cloud)), alpha=0.5) + a + c
ggplot(train) + geom_density(aes(x=CORR, group=factor(label), fill=factor(cloud)), alpha=0.5) + a + c
# 
```

Figure 2: Class conditional density plots of the training data on the five camera angles and SD, CORR, and NDAI

After some preliminary exploratory data analysis, it quickly became clear that our first major decision would be to decide what to do with the portions of the image where the expert was uncertain whether it was a cloud or not. As a result, our figures and plots show both options. However, after deliberation, it was decided that the zero's (Unlabeled) data should be removed. This makes sense because a model should not work to predict unlabeled areas of the image. Instead it should give a prediction of cloud or not cloud and then give some measure of confidence.

Once this decision was made, we were able to look at the relationship between the radiances of different camera angles. After looking at the correlations between each of the five camera angles we can see that the radiances of cameras closest to each other are much more similar to each other than to ones with drastically different angles. When the focus is shifted to the label and the radiances we see that the range of radiances is much larger for clouds than for not cloud. Furthermore for all camera angles we notice a bi-modal distribution of radiances for not cloud. Finally we looked to see if there was a difference based on the features NDAI, SD, and CORR. From the correlation plot above we can see that all three features' correlations consistently become more negative as the camera angles face more directly downward. However, CORR has the largest changes in correlation between camera angles, going from positive in DF, to the strongest negative correlation of all the features in AN.

#3. Modeling

##3.1 Feature Selection
To predict what are the three best features to predict the presence of clouds, we used several methods. The first and simplest was to look at the heat map and the conditional density plots. From these diagrams, we came to the conclusion that NDAI, CORR, and AF were the best at predicting the truth. NDAI was the clear favorite visually. It had the strongest blue color of all of the features in the heat map (Figure 1) and it had clear separation in the density plot in Figure 3. CORR seemed to have a similar correlation with SD based on the heat map so it was necessary to observe the density plots to make a final decision. From these plots it seemed that CORR had more separated peaks than SD. Finally, AF was selected as the radiance angle. Looking at the heat map, AF and AN seemed to be the same color with a negative correlation so it was necessary to compare their density plots. Again these seemed very close. However it looked like AF had a slightly larger green (cloud) tail so we decided upon AF over AN.

The second method that was used to identify the three best features was to simply look at the correlation between label and the remaining eight features. These numbers can be seen in Figure 1. Selecting the three variables with the three largest correlations would result in NDAI, CORR, and AF being selected. Choosing NDAI and CORR was simple because they had the two largest correlations. However deciding between AF and AN was difficult. Since the correlations with label are the same, ideally we would choose the one that was least correlated with NDAI and CORR. In this situation NDAI is more correlated with AF than AN and CORR is more correlated with AN than AF. As a result we decided to choose AF due to its larger difference in correlation with CORR.

Finally, we try to use a measure that is more relevant to binary variable. In term of performance measure, accuracy is a simple yet effective measurement. It is symmetric with respect to the zero and one class, unlike some other measurements with emphasizes one class more than the other. In our case, the problem of detecting cloud and no cloud is symmetric, so accuracy is an appropriate measure. Accuracy simply means the percentage of time a model classifies correctly. The only downside is that most models return a probability based prediction, and accuracy depends on the threshold at which one cut the probability to classify as positive and negative. One way to fix this dependence on threshold is to pick the threshold with the best possible accuracy. 

The other way is to use area under curve (AUC) of the ROC curve. This measure is independent of threshold. The continuous variable (predictor) does not need to be between 0 and 1. On the down side, AUC is hard to generalize for the case of more than two classes. Also, naive AUC calculation that is based on rectangular approximation can be slow, at $O(n^2)$. If we use the probability based method, and sort the data with respect to the continuous variable, we can get $O(n\log n)$ time. With the AUC approach, we can pick the three inputs with the highest AUC with respect to the training label. The AUC for each input with respect to the label for the all three images (after getting rid of zero labels) is as followed:

```{r kable, results="asis", echo = FALSE, fig.cap = "AUC of inputs with respect to output"}
GetAUCTable()
```

Table: AUC of inputs with respect to output

Note that AUC is a measurement of between 0 and 1, which 1 can be thought of perfect positive correlation, 0 is similar to perfect negative correlation, and 0.5 means no correlation. Based on the table, we would pick the three most "correlated" inputs, which are NDAI, SD, and CORR. AF's performance is quite closed to that of CORR, as AUC 0.1858 is equivalent (opposite sign) of 1 - 0.1858 = 0.8142. 

Throughout this paper, we will use AUC as the main measurement for to choose the best model. When we need to get the actual predicted value of 0 and 1 instead of continuous predicted value (e.g. predicted probability), we will use a threshold, and measure performance by accuracy. We also note that just using raw NDAI as a predictor for classifying cloud and no cloud, the AUC is already 0.9344. Any model that does not perform as well as this very naive approach should be discarded. And if fact as we see later, many of the simple models only have performance slightly higher than that benchmark. Put in another perspective, the scientists who designed this NDAI signal have done a great job transforming somewhat weak radiances signal into a powerful signal. 

##3.2. Overview of Classifiers
When solving a classification problem, we are presented with an abundance of choices to make. Following is a broad breakdown by:

1. Model 
+ Linear Regression 
+ Logistic Regression
+ LDA, QDA
+ SVM
+ naiveBayes
+ randomForest
+ neural network
2. Feature Engineering
  + Include polynomial term, interactive term, e.g. $x_i ^2, x_i x_j$
  + Log-Rescale, squareroot rescale: $sign(x) \log (|x|+1)$, $sign(x) \sqrt(|x|)$
3. Regularization
  + L1 loss, L1 then OLS on selected variables, OLS then L1 on selected variables
  + L2 loss
  + L1 + L2 (Elastic Net)
  + Adaptive L1 (weighted L1)
  + Forward stepwise, backward stepwise selection
4. Model Selection, Choosing Model Parameter
  + Cross Validation
  + AIC, AICc, BIC
5. Performance Measure
  + AUC
  + Accuracy
  + Logloss, deviance, mutual information
  + F1 Score, Mean Average Precision, Cohen's Kappa
6. Optimization Algorithm
  + Gradient Descent family: Stochastic Gradient Descent, Coordinate Descent
  + Newton Method family: Quasi-Newton, BFGS
  + LARS (for L1 and Elastic Net)
  
Of course, not all combination are possible. For example, LAR algorithm is only applied for L1 and Elastic Net regularization. Still, we are left with a very wide range of options to choose from. For the scope of this lab, we won't have time to study and implement all the possible combination, and so we heuristically restrict ourselves to some specific set of options. 

For most of the model, we try out of the box implementation with out much calibration. We pay more attention to Logistic Regression, and SVM in particular, which represent the statistical approach, and optimization approach to classification respectively. For Logistic Regression, we try polynomial and interactive terms, L1 regularization with cross validation as the tool for picking the best regularization. Cross validation uses Area Under Curve of ROC curve as the measurement. We use the "glmnet" package, which implements Coordinate Descent algorithm. Following is the list of models that we run on our dataset:

Model Specification:

1. Linear Regression: The response variable (binary 0 and 1 in our case) is a  linear function of X with white noise. 
2. Logistic Regression: Condition on X, the log odd is linear function of X.
3. naiveBayes: The input X's are conditional independent given Y.
4. Quadratic Discriminant Analysis: X for each group is Multivariate Gaussian
5. Neural Network: 1 hidden layers with 10 hidden nodes
6.  Random Forest: 640 trees
7. Logistic with L1 Loss, CV on AUC
8. Logistic with interactive terms
9. Support Vector Machine: with optimized cost and gamma parameter

Some of the models are more of optimization procedures than statistical models, namely Neural Network, Random Forest (decision tree), and Support Vector Machine. As such there are really no assumption. We instead check the model assumption for the probabilistic models. For Linear Regression, it is clear that the assumption will not be met for binary responses. However, as we will see Linear Regression thought of as a Least Square method can still perform very well. 

To illustrate the differences among classification algorithms, we borrow this image from Mark Landry. The two missing algorithms in his list that we did not get to try are Nearest Neighbor and Gradient Boosted Machines. 

```{r, echo = FALSE, fig.cap="Classification Algorithms. Owner: Mark Landry. Url: http://www.slideshare.net/mark_landry/gbm-package-in-r", message = FALSE, fig.width=3, fig.height=3}
library(grid)
img = readJPEG("list.algorithm.jpg")
grid.raster(img)
```

We check the model assumption for Logistic Regression. The log-odd should be linear in each of the inputs. 

```{r,echo=FALSE, fig.width = 3.5, fig.height = 3.5, message = FALSE, fig.cap = "Plot of Log-odd for each bucket with respect to input"}
source("PerformanceMetrics.R")
logreg.fit = glm(label ~ NDAI + SD + CORR + DF + CF +
                              BF   + AF + AN,
                      data = train, family = binomial(link = "logit"))

label.hat2 = predict(logreg.fit, train, type = "link")

plotLogoddTruthPreds(train$label, train$NDAI, .xlab="NDAI")
plotLogoddTruthPreds(train$label, train$SD, .xlab="SD")
plotLogoddTruthPreds(train$label, train$CORR, .xlab = "CORR")
plotLogoddTruthPreds(train$label, label.hat2, .xlab = "Yhat")
```

Figure: Log-odd plot condition on inputs. For checking Logistic Regression assumption

Looking at the plots of log-odd (in buckets) versus each of the inputs, we see that the plot of log odd with respect to NDAI, SD, and CORR are not linear. Including quadratic terms would help. This explains why we see a higher performance in QDA, or non-linear methods such as random forest and neural network. 

For naive Bayes, condition on the label equal 1, we have the correlation of inputs are:


```{r kable2, results="asis", echo = FALSE, fig.cap = "Correlation matrix condition on  there is cloud, for checking naive Bayes assumption"}
GetCorTable()
```

It is clearly that the correlation are quite high, thus the assumptions are not met. But still naive Bayes method often performs quite well even when the assumptions are not met. 

For QDA, we need the inputs to be Gaussian condition on the class. Looking at the marginal Q-Q plot with respect to the normal quantiles, we see that none of the inputs have a linear Q-Q plot. So again the assumptions are not met.

```{r,echo=FALSE, dev="png",fig.cap="Normal QQ plot of inputs condition on Label = 0. For checking QDA Gaussian assumption"}
par(mfrow=c(2,2))
qqnorm(train$NDAI[train$label == 0], ylab = "NDAI")
qqnorm(train$SD[train$label == 0], ylab = "SD")
qqnorm(train$CORR[train$label == 0], ylab = "CORR")
qqnorm(train$AN[train$label == 0], ylab = "AN")
```

## 3.3. Cross Validation Result for Different Classifications
For our data, the rows are not i.i.d. As such we have to be more careful in choosing the train set, test set, and cross validation sets. Our end goal in this data problem is to be able to classify cloud and no cloud in new images. We only have three images so one way to create more observations is to divide each image into k by k smaller images. Doing this, each block can be thought of as a separate image, and we have $3k^2$ images. These newly created images are not totally independent; still, dividing three images into small images should help us in building a more stable model on new images.

In our data, we choose k = 3, as such there are 27 small images. We choose 15 blocks at random to use as train, and leave the remaining as test. We do this 200 times, each time choosing 15 blocks at random, calculating the AUC of the predictor with respect to the label in test set. The result is reported in the box plot below.  

```{r, echo=FALSE,fig.height = 4.5 ,fig.cap = "Classification Algorithm AUC Performance "}
load("./LinearModel/LinearModel.RData")
LinearModelAUC = unlist(out)
run.output = readLines("./LinearModel/LinearModel.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df1 = data.frame(model = rep("Linear", length(LinearModelAUC)), 
                AUC = LinearModelAUC, run.time = run.time.elapsed)
#################
load("./LogisticModel/LogisticModel.RData")
LogisticModelAUC = unlist(out)
run.output = readLines("./LogisticModel/LogisticModel.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df2 = data.frame(model = rep("Logistic", length(LogisticModelAUC)), 
                AUC = LogisticModelAUC, run.time = run.time.elapsed)
#################
load("./naiveBayes/naiveBayes.RData")
naiveBayesAUC = unlist(out)
run.output = readLines("./naiveBayes/naiveBayes.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df3 = data.frame(model = rep("nBayes", length(out)),
                 AUC = naiveBayesAUC, run.time = run.time.elapsed)
#################
load("./QDA/QDA.RData")
QDAAUC = unlist(out)
run.output = readLines("./QDA/QDA.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df4 = data.frame(model = rep("QDA", length(QDAAUC)),
                AUC = QDAAUC, run.time = run.time.elapsed)
##################
load("./NeuralNet/NeuralNet.RData")
NeuralNetworkAUC = sapply(out, unlist)[8,]
run.output = readLines("./NeuralNet/NeuralNet.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df5 = data.frame(model = rep("NNet", length(out)),
                 AUC = NeuralNetworkAUC, run.time = run.time.elapsed)
##################
load("./RandomForest/RandomForest.RData")
run.output = readLines("./RandomForest/RandomForest.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df6 = data.frame(model = rep("RForest", nrow(RandomForestAUC)),
              AUC = RandomForestAUC[,7], run.time = run.time.elapsed)


##################
load("./LogitCV/LogitCV.RData")
LogitCVAUC = unlist(out)
run.output = readLines("./LogitCV/LogitCV.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df7 = data.frame(model = rep("LogitCV", length(out)),
                 AUC = LogitCVAUC, run.time = run.time.elapsed)

###################
load("./LogitPoly/LogitPoly.RData")
LogitPolyAUC = unlist(out)
run.output = readLines("./LogitPoly/LogitPoly.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df8 = data.frame(model = rep("LogitPoly", length(out)),
                 AUC = LogitPolyAUC, run.time = run.time.elapsed)

#####################
load("./SVM/SVM.RData")
run.output = readLines("./SVM/SVM.out")
run.time = run.output[length(run.output)]
run.time = strsplit(run.time, split = " ")[[1]]
run.time.elapsed = as.numeric(run.time[length(run.time)])
df10 = data.frame(model = rep("SVM", length(SVMAUC)),
                  AUC = SVMAUC, run.time = run.time.elapsed)

df = rbind(df1, df2, df3, df4, df5, df6, df7, df8, df10)
ggplot(data = df, aes(x = model, y = AUC)) + geom_boxplot() + coord_flip()

```
```{r kable3, results="asis", echo = FALSE, fig.cap ="Mean AUC for each Algorithm, and running time in second."}
dfmean = summarise(group_by(df, model), mean(AUC), mean(run.time), count = n())
models.Perf = train[1:2,1:9] # Create a dataframe of 9 column, 2 row
names(models.Perf) = dfmean$model
models.Perf[1,] = dfmean[,"mean(AUC)"]
models.Perf[2,] = dfmean[,"mean(run.time)"]/dfmean[,"count"]
row.names(models.Perf) = c("mean AUC","Run-time (s)")
kable(models.Perf, digits = 3)
```


We see that random forest have the highest performance, followed by logistic with interactive terms, neural network, and QDA. The class of simple models namely linear model, logistic model, and naive Bayes are not as good but not too far behind. Regularization does not help logistic regression much, as we only have 7 variable and 100,000 observations, so there is not much overfitting. 

We see that in general simple methods run much faster. QDA seems to have a good combination of performance and computational cost. The run time is approximate. 
For each model we run 200 times on cluster, and take total elapsed time divided by the number of models ran (200). But for some model we used 10 cores, some 30 cores
because of the memory limit (e.g. complex model such as Random Forest must be ran using < 10 cores while Linear Model can be ran with 32 cores). As such the time is not 
very comparable, but still provides a heuristic estimate.

##3.4. Convergence of Parameter Estimation

Since random forest model does not really return any meaningful parameters, we will work on the logistic regression model for this subsection. We will look at the estimated $\beta$, when using one block to train, two blocks, and so on until we use all 27 blocks to train the logistic model. The inputs are standardized to zero mean and unit variance. 

```{r, echo = FALSE, fig.cap = "Convergence of betahat for Logistic Regression when training size increases"}
load("./LogitConvergence/LogitConvergence.RData")
beta.hat = sapply(out, unlist)
par(mfrow=c(3,3))
xnames = names(train)[3:11]; xnames[1] = "(Intercept)"
for (col.name in xnames)
{
  plot(beta.hat[col.name,1:108], xlab = "", ylab = col.name, type = "l", ylim = c(-4, 10))  
}
```


We see that the model is quite stable with respect to adding more blocks into the training data. 



## 3.5. Misclassification Error
### 3.5.1. By image region
We first see misclassification error with respect to region in the image. We run Random Forest model on 15 blocks and then use the model to predict both the training and testing data. The training region is in the black boxes. We include training regions first for a comparison of how models work in sample and out sample, and to make the image complete. We do the same for Logistic Model, as a comparision to the more complex Random Forest. The blue signifies cloud and red signifies no cloud. The three images on the left are for random forest, and the three images on the right are for logistic regression. 


```{r, echo = FALSE, message = FALSE, fig.height = 3, fig.width = 4}
logreg.fit = glm(label ~ NDAI + SD + CORR + DF + CF +
                              BF   + AF + AN,
                      data = train, family = binomial(link = "logit"))
logistic.yhat = predict(logreg.fit, rbind(train,test), type = "response")
library(randomForest)
forest.fit = randomForest(factor(label) ~ NDAI + SD + CORR + DF +
                            CF + BF + AF + AN,
                          data = train,
                          ntree = 160)
random.forest.yhat = predict(forest.fit, rbind(train,test), type = "prob")

yhat1 = cutOff(random.forest.yhat[,2])
yhat2 = cutOff(logistic.yhat)
plot.missclassified(img.id = 1, preds = yhat1)
plot.missclassified(img.id = 1, preds = yhat2)
plot.missclassified(img.id = 2, preds = yhat1)
plot.missclassified(img.id = 2, preds = yhat2)
plot.missclassified(img.id = 3, preds = yhat1)
plot.missclassified(img.id = 3, preds = yhat2)
```

Figure: Left - Classification Error for Random Forest. Right - Classification Error for Logistic Regression.
Regions inside blackbox are training data. Region outside blackbox are test data. 

For both model, we can see that our models seem to do a pretty good job of predicting. However there are a few areas where we run into problems. One consistent problem is our model predicting clouds on land that is on the edge of the unknown (the white areas). However, once we get to the middle of a big cloud or a big no cloud area, our models do a very good job of consistently predicting correctly. Another problem is that it was trying to predict land in the middle of clouds. There seems to be a scattershot of error throughout most large clouds where the model sprinkles land throughout a cloud. 

One very interesting feature of random forest is that it almost did not make any errors in the training blocks. This may be because Random Forests can overfit to the training data. As a decision tree model, Random Forest can easily fit the training data perfectly. Logistic regression as expected have equally good performances in and out of sample. 

### 3.5.2. By range of input

```{r kable4, results="asis", echo = FALSE, fig.cap ="Misclassification of Logistic Model by range of inputs"}

label.hat2 = predict(logreg.fit, test, type = "response")
kable(misclassfication.matrix(cutOff(label.hat2), test), digit = 2)
```

Table: Table of Classification Error for Logistic Model. The row are quartile of data. For example, the top row entry in the column of NDAI means for the lowest quartile of NDAI value, Logistic Model misclassification rate is 0.00.  

From the table, we see that when SD is high, Logistic Model performs worse. These mean when the radiances of different angles around one pixel are very different, and correlation are very low, our model perform worse. We are not sure why this is the case. For the radiances, when the radiances are low, which means weak light, our model performs worse. This meets our expectation as when cloud is high, it is easier to predict cloud, and high cloud means high radiances. 

We also provide the table for Random Forest. 

```{r kable5, results="asis", echo = FALSE, message = FALSE, fig.cap ="Misclassification of Logistic Model by range of inputs"}
label.hat7 = predict(forest.fit, test, type = "prob")
kable(misclassfication.matrix(cutOff(label.hat7[,2]), test), digit = 2)
```

Table: Table of Classification Error for Random Forest. The rows are quartile of data. For example, the top row entry in the column of NDAI means for the lowest quartile of NDAI value, Logistic Model misclassification rate is 0.00.   

We see quite similar pattern of misclassification for Random Forest. It seems to not perform well for high NDAI, high SD, and low radiances. Exception is CORR where Random Forest does not perform well for high CORR. 

## 3.6. Future Data
We believe that our model should perform well in out of sample data, as long as the new images are taken from similar regions. This is because we randomly select a different blocks of images as train and test out of sample performance. We also did this multiple times (200) to increase our confidence in out of sample performance. For Random Forest, it should perform better with regularization, as we saw that in sample it overfits. 

# 4. Conclusion

When trying to solve climate change issues an important thing is to look at the cloud cover on earth. The current algorithms that we have work well with most land and ocean geography, however issues arise when trying to identify cloud cover over polar regions. This is due to ice and snow have similar radiances to clouds. As a result to do this identification we require experts to do a manual classification for all the satellite images. This is very time intensive and shows a concrete need for an algorithm that does a good job distinguishing between these geographic features. 

To solve this problem, we applied a multitude of models to a training set from the MISR data. These models include Linear Regression, Logistic Regression, naiveBayes, QDA, NN, Random Forest, Logistic and SVM. Once we trained our models we used the remaining data (test data) to determine various measures of performance (accuracy, AUC, F1 score, logloss and others). We found that the Random Forest was the most effective model, predicting approximately 97 percent. This high percentage combined with its relatively short running time made it the best choice for cloud identification. Areas of concern would be the edges of no clouds next to unknown areas and scattershot through cloud areas. With regards to the range intervals, we have noticed that the higher ranges of NDAI, CORR and SD have a higher probability of misclassification relative to other ranges. The camera angles follow the opposite pattern. 

Our model should perform at a similar rate assuming that the image was taken from a similar geographic region. Ideally we would have an expert labelled image from most geographic regions and our classifier would work at comparable levels of performance.

###5. References

Hastie, Trevor, et al. The elements of statistical learning. Vol.2. No.1.New York:Springer,2009

Landry, Mark. http://www.slideshare.net/mark_landry/gbm-package-in-r. 2014.

Shi, Tao, et al. "Daytime arctic cloud detection based on multi-angle satellite data with case studies." Journal of the American Statistical Association 103.482 (2008): 584-593.

#####6.Project GitHub Repo 

https://github.com/RahulJKVerma/Stats-215---Lab-4 

